---
phase: 07-output-unification
plan: 04
type: execute
wave: 3
depends_on: [07-01, 07-02, 07-03]
files_modified: [tests/test_output_unification.py, README.md]
autonomous: true

must_haves:
  truths:
    - "Tests verify stderr routing for logger messages"
    - "Tests verify --quiet flag suppresses progress but not data"
    - "Tests verify unified header format in both modes"
    - "Tests verify statistics footer appears in compare mode"
    - "README documents --quiet flag usage"
  artifacts:
    - path: "tests/test_output_unification.py"
      provides: "Test coverage for Phase 7 features"
      min_lines: 100
    - path: "README.md"
      provides: "Documentation for --quiet flag"
      contains: "--quiet"
  key_links:
    - from: "test_output_unification.py"
      to: "file_matcher.py"
      via: "subprocess.run testing CLI"
---

<objective>
Add comprehensive tests for output unification features and document --quiet flag

Purpose: Ensure Phase 7 features (stream separation, --quiet, unified headers, statistics footer) are thoroughly tested and documented. Tests use subprocess for true CLI behavior verification.

Output: New test file tests/test_output_unification.py and updated README.md
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-output-unification/07-CONTEXT.md
@.planning/phases/07-output-unification/07-01-SUMMARY.md
@.planning/phases/07-output-unification/07-02-SUMMARY.md
@.planning/phases/07-output-unification/07-03-SUMMARY.md
@tests/test_base.py (for test patterns)
@tests/test_json_output.py (for subprocess test patterns)
@README.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create test_output_unification.py with stream separation tests</name>
  <files>tests/test_output_unification.py</files>
  <action>
  Create a new test file with tests for Phase 7 features. Follow patterns from test_json_output.py for subprocess-based testing.

  ```python
  """Tests for Phase 7: Output Unification features.

  Tests cover:
  - Stream separation (logger to stderr, data to stdout)
  - --quiet flag behavior
  - Unified header format
  - Statistics footer in compare mode
  """
  from __future__ import annotations

  import subprocess
  import sys
  import unittest
  from pathlib import Path


  class TestStreamSeparation(unittest.TestCase):
      """Tests for stdout/stderr stream separation."""

      def setUp(self):
          """Set up test directories."""
          self.test_dir1 = str(Path(__file__).parent.parent / "test_dir1")
          self.test_dir2 = str(Path(__file__).parent.parent / "test_dir2")

      def test_logger_messages_go_to_stderr(self):
          """Verify logger messages (Using MD5...) go to stderr, not stdout."""
          result = subprocess.run(
              [sys.executable, "file_matcher.py", self.test_dir1, self.test_dir2],
              capture_output=True,
              text=True
          )
          # Logger messages should be on stderr
          self.assertIn("Using MD5", result.stderr)
          # Data should be on stdout
          self.assertIn("Hash:", result.stdout)
          # Logger messages should NOT be on stdout
          self.assertNotIn("Using MD5", result.stdout)

      def test_stderr_with_verbose_mode(self):
          """Verify verbose progress goes to stderr."""
          result = subprocess.run(
              [sys.executable, "file_matcher.py", self.test_dir1, self.test_dir2, "--verbose"],
              capture_output=True,
              text=True
          )
          # Verbose progress should be on stderr
          self.assertIn("Indexing", result.stderr)

      def test_errors_go_to_stderr(self):
          """Verify error messages go to stderr."""
          result = subprocess.run(
              [sys.executable, "file_matcher.py", "nonexistent", "alsonothere"],
              capture_output=True,
              text=True
          )
          self.assertIn("Error", result.stderr)
          self.assertEqual(result.returncode, 1)


  class TestQuietFlag(unittest.TestCase):
      """Tests for --quiet/-q flag."""

      def setUp(self):
          """Set up test directories."""
          self.test_dir1 = str(Path(__file__).parent.parent / "test_dir1")
          self.test_dir2 = str(Path(__file__).parent.parent / "test_dir2")

      def test_quiet_suppresses_progress(self):
          """--quiet should suppress progress messages."""
          result = subprocess.run(
              [sys.executable, "file_matcher.py", self.test_dir1, self.test_dir2, "--quiet"],
              capture_output=True,
              text=True
          )
          # Should not have logger messages
          self.assertNotIn("Using MD5", result.stderr)
          self.assertNotIn("Indexing", result.stderr)
          # Should still have data
          self.assertIn("Hash:", result.stdout)

      def test_quiet_short_flag(self):
          """-q should work same as --quiet."""
          result = subprocess.run(
              [sys.executable, "file_matcher.py", self.test_dir1, self.test_dir2, "-q"],
              capture_output=True,
              text=True
          )
          self.assertNotIn("Using MD5", result.stderr)

      def test_quiet_still_shows_data(self):
          """--quiet should not suppress data output."""
          result = subprocess.run(
              [sys.executable, "file_matcher.py", self.test_dir1, self.test_dir2, "--quiet"],
              capture_output=True,
              text=True
          )
          # Data output should still appear
          self.assertIn("Hash:", result.stdout)

      def test_quiet_still_shows_errors(self):
          """--quiet should not suppress error messages."""
          result = subprocess.run(
              [sys.executable, "file_matcher.py", "nonexistent", "alsonothere", "--quiet"],
              capture_output=True,
              text=True
          )
          # Errors should still appear
          self.assertIn("Error", result.stderr)

      def test_quiet_with_action_mode(self):
          """--quiet should work in action mode."""
          result = subprocess.run(
              [sys.executable, "file_matcher.py", self.test_dir1, self.test_dir2,
               "--action", "hardlink", "--quiet"],
              capture_output=True,
              text=True
          )
          self.assertNotIn("Using MD5", result.stderr)
          # Should still show action preview
          self.assertIn("MASTER", result.stdout)


  class TestUnifiedHeaders(unittest.TestCase):
      """Tests for unified header format."""

      def setUp(self):
          """Set up test directories."""
          self.test_dir1 = str(Path(__file__).parent.parent / "test_dir1")
          self.test_dir2 = str(Path(__file__).parent.parent / "test_dir2")

      def test_compare_mode_header(self):
          """Compare mode should show 'Compare mode: dir1 vs dir2'."""
          result = subprocess.run(
              [sys.executable, "file_matcher.py", self.test_dir1, self.test_dir2],
              capture_output=True,
              text=True
          )
          self.assertIn("Compare mode:", result.stdout)
          self.assertIn("vs", result.stdout)

      def test_action_mode_preview_header(self):
          """Action mode preview should show 'Action mode (PREVIEW): action'."""
          result = subprocess.run(
              [sys.executable, "file_matcher.py", self.test_dir1, self.test_dir2,
               "--action", "hardlink"],
              capture_output=True,
              text=True
          )
          self.assertIn("Action mode (PREVIEW)", result.stdout)
          self.assertIn("hardlink", result.stdout)

      def test_quiet_suppresses_header(self):
          """--quiet should suppress unified header."""
          result = subprocess.run(
              [sys.executable, "file_matcher.py", self.test_dir1, self.test_dir2, "--quiet"],
              capture_output=True,
              text=True
          )
          # Header should not appear
          self.assertNotIn("Compare mode:", result.stdout)


  class TestCompareStatisticsFooter(unittest.TestCase):
      """Tests for statistics footer in compare mode."""

      def setUp(self):
          """Set up test directories."""
          self.test_dir1 = str(Path(__file__).parent.parent / "test_dir1")
          self.test_dir2 = str(Path(__file__).parent.parent / "test_dir2")

      def test_compare_mode_shows_statistics(self):
          """Compare mode should show statistics footer."""
          result = subprocess.run(
              [sys.executable, "file_matcher.py", self.test_dir1, self.test_dir2],
              capture_output=True,
              text=True
          )
          self.assertIn("--- Statistics ---", result.stdout)
          self.assertIn("Duplicate groups:", result.stdout)

      def test_statistics_after_match_groups(self):
          """Statistics should appear after match groups."""
          result = subprocess.run(
              [sys.executable, "file_matcher.py", self.test_dir1, self.test_dir2],
              capture_output=True,
              text=True
          )
          # Find positions
          hash_pos = result.stdout.find("Hash:")
          stats_pos = result.stdout.find("--- Statistics ---")
          # Statistics should be after hash output
          self.assertGreater(stats_pos, hash_pos)

      def test_json_compare_includes_statistics(self):
          """JSON compare mode should include statistics object."""
          import json
          result = subprocess.run(
              [sys.executable, "file_matcher.py", self.test_dir1, self.test_dir2, "--json"],
              capture_output=True,
              text=True
          )
          data = json.loads(result.stdout)
          self.assertIn("statistics", data)
          self.assertIn("duplicateGroups", data["statistics"])


  if __name__ == "__main__":
      unittest.main()
  ```
  </action>
  <verify>
  Run: `python3 -m tests.test_output_unification`
  Expected: All tests pass (assuming Plans 01-03 implemented correctly)
  </verify>
  <done>
  test_output_unification.py exists with comprehensive tests for stream separation, --quiet flag, unified headers, and statistics footer.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update README with --quiet flag documentation</name>
  <files>README.md</files>
  <action>
  Update the README.md to document the --quiet flag and explain the stream separation behavior.

  Find the Options section (or CLI arguments section) and add --quiet:

  ```markdown
  ### Output Control

  - `--quiet, -q` - Suppress progress messages and headers. Only output data (match groups, statistics). Useful for scripting and piping. Error messages still appear.
  ```

  Also add a section explaining stdout/stderr behavior if not present:

  ```markdown
  ### Output Streams

  File Matcher follows Unix conventions for output streams:

  - **stdout**: Data output (match groups, statistics, JSON)
  - **stderr**: Progress messages, status updates, errors

  This enables clean piping:
  ```bash
  # Pipe only data to grep
  filematcher dir1 dir2 | grep "pattern"

  # Redirect data to file, see progress on terminal
  filematcher dir1 dir2 > matches.txt

  # Suppress all progress with --quiet
  filematcher dir1 dir2 --quiet | wc -l
  ```
  ```

  Place these in logical locations within the existing README structure.
  </action>
  <verify>
  Run: `grep -A2 -- "--quiet" README.md`
  Expected: Shows --quiet documentation

  Run: `grep "stderr" README.md | head -3`
  Expected: Shows explanation of stderr usage
  </verify>
  <done>
  README.md documents --quiet flag and explains stdout/stderr stream separation.
  </done>
</task>

<task type="auto">
  <name>Task 3: Run full test suite and verify all tests pass</name>
  <files>run_tests.py</files>
  <action>
  Run the complete test suite including the new output unification tests.

  Update run_tests.py if needed to include the new test module. Check if it auto-discovers tests or needs explicit module listing.

  ```bash
  python3 run_tests.py
  ```

  If run_tests.py uses test discovery, no changes needed. If it lists modules explicitly, add:
  ```python
  'tests.test_output_unification',
  ```

  Verify:
  - All existing tests pass (no regressions)
  - New output unification tests pass
  - Total test count increased by ~12-15 tests
  </action>
  <verify>
  Run: `python3 run_tests.py`
  Expected: All tests pass, including new test_output_unification tests

  Run: `python3 run_tests.py 2>&1 | grep -E "^(Ran|OK|FAILED)"`
  Expected: Shows "Ran N tests" with OK status
  </verify>
  <done>
  Full test suite passes including new output unification tests. Test count reflects new coverage.
  </done>
</task>

</tasks>

<verification>
Overall phase checks:

1. New tests exist and pass:
   - `python3 -m tests.test_output_unification` shows all tests passing
   - Tests cover: stream separation, --quiet, headers, statistics

2. Full test suite passes:
   - `python3 run_tests.py` exits 0
   - No regressions in existing tests

3. README updated:
   - --quiet flag documented
   - stdout/stderr behavior explained

4. Test coverage meaningful:
   - Tests use subprocess for true CLI behavior
   - Tests verify both stdout and stderr content
   - Tests verify --quiet suppression behavior
</verification>

<success_criteria>
- tests/test_output_unification.py exists with 12+ tests
- Tests cover stream separation (stderr routing)
- Tests cover --quiet flag (suppression, still shows data, still shows errors)
- Tests cover unified headers (compare mode, action mode preview)
- Tests cover statistics footer in compare mode
- README documents --quiet flag
- README explains stdout/stderr stream separation
- All tests pass (existing + new)
</success_criteria>

<output>
After completion, create `.planning/phases/07-output-unification/07-04-SUMMARY.md`
</output>
