---
phase: 02-dry-run-preview
plan: 04
type: execute
wave: 3
depends_on: ["02-03"]
files_modified:
  - tests/test_dry_run.py
autonomous: true

must_haves:
  truths:
    - "Tests verify dry-run banner appears in output"
    - "Tests verify statistics footer with correct counts"
    - "Tests verify [DUP:?] and [DUP:action] labels"
    - "Tests verify cross-filesystem warning appears"
    - "Tests verify --dry-run requires --master validation"
  artifacts:
    - path: "tests/test_dry_run.py"
      provides: "Dry-run output unit tests"
      min_lines: 150
  key_links:
    - from: "tests/test_dry_run.py"
      to: "file_matcher.main()"
      via: "sys.argv patching and output capture"
      pattern: "patch.*sys.argv"
---

<objective>
Create unit tests for dry-run output formatting (TEST-02)

Purpose: Comprehensive test coverage ensures dry-run functionality works correctly and prevents regressions. This delivers requirement TEST-02.

Output: New test file tests/test_dry_run.py with tests for banner, statistics, action labels, and cross-filesystem warnings.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-dry-run-preview/02-03-SUMMARY.md
@file_matcher.py
@tests/test_base.py
@tests/test_cli.py
@tests/test_master_directory.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create test file with banner and validation tests</name>
  <files>tests/test_dry_run.py</files>
  <action>
Create `tests/test_dry_run.py` following existing test patterns (test_cli.py, test_master_directory.py):

```python
"""Unit tests for dry-run output formatting."""
import os
import sys
import tempfile
from io import StringIO
from unittest.mock import patch
from contextlib import redirect_stdout, redirect_stderr

from tests.test_base import BaseFileMatcherTest
import file_matcher


class TestDryRunValidation(BaseFileMatcherTest):
    """Tests for --dry-run flag validation."""

    def test_dry_run_requires_master(self):
        """--dry-run without --master should fail with error."""
        # Test that error message contains "requires --master"

    def test_dry_run_with_master_succeeds(self):
        """--dry-run with --master should not produce validation error."""

    def test_action_choices_valid(self):
        """--action accepts hardlink, symlink, delete."""

    def test_action_invalid_choice(self):
        """--action with invalid choice should fail."""


class TestDryRunBanner(BaseFileMatcherTest):
    """Tests for dry-run banner output."""

    def test_banner_displayed_at_top(self):
        """Dry-run banner should appear at start of output."""
        # Verify "DRY RUN" and "No changes will be made" in output
        # Verify it's at the top (first non-empty line)

    def test_banner_not_shown_without_dry_run(self):
        """Banner should not appear when --dry-run not specified."""

    def test_banner_shown_with_summary(self):
        """Banner should appear even with --summary flag."""
```

Use `redirect_stdout()` and `redirect_stderr()` to capture output. Use `patch('sys.argv', [...])` to set arguments.
  </action>
  <verify>
Run:
```bash
python3 -m tests.test_dry_run 2>&1 | head -20
```
Should show test discovery and initial test results (may have failures until Task 2 complete).
  </verify>
  <done>
tests/test_dry_run.py exists with TestDryRunValidation and TestDryRunBanner test classes. Tests follow existing patterns from test_cli.py.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add statistics and action label tests</name>
  <files>tests/test_dry_run.py</files>
  <action>
Add more test classes to tests/test_dry_run.py:

```python
class TestDryRunStatistics(BaseFileMatcherTest):
    """Tests for dry-run statistics footer."""

    def test_statistics_footer_displayed(self):
        """Statistics section should appear at end of output."""
        # Verify "Statistics" header
        # Verify "Duplicate groups:" line
        # Verify "Duplicate files:" line
        # Verify "Space to be reclaimed:" line

    def test_statistics_counts_correct(self):
        """Statistics should show correct counts."""
        # Set up known duplicates, verify numbers match

    def test_verbose_shows_exact_bytes(self):
        """Verbose mode should show exact byte count."""
        # Verify "(X bytes)" format in verbose output

    def test_summary_shows_only_statistics(self):
        """--dry-run --summary should show only stats, no file list."""
        # Verify no [MASTER]/[DUP] lines, but stats present


class TestDryRunActionLabels(BaseFileMatcherTest):
    """Tests for action labels in dry-run output."""

    def test_no_action_shows_question_mark(self):
        """Without --action, duplicates show [DUP:?]."""
        # Verify "[DUP:?]" in output

    def test_hardlink_action_label(self):
        """With --action hardlink, duplicates show [DUP:hardlink]."""

    def test_symlink_action_label(self):
        """With --action symlink, duplicates show [DUP:symlink]."""

    def test_delete_action_label(self):
        """With --action delete, duplicates show [DUP:delete]."""


class TestDryRunCrossFilesystem(BaseFileMatcherTest):
    """Tests for cross-filesystem warnings."""

    def test_cross_fs_warning_in_output(self):
        """Cross-filesystem files should show [!cross-fs] warning."""
        # This test may be difficult to set up without actual cross-fs
        # Consider mocking check_cross_filesystem instead

    def test_cross_fs_count_in_statistics(self):
        """Statistics should show count of cross-fs files when present."""
```

For cross-filesystem tests, use `unittest.mock.patch` to mock `check_cross_filesystem()` to return a known set of "cross-filesystem" files, since setting up actual cross-filesystem scenarios is environment-dependent.
  </action>
  <verify>
Run:
```bash
python3 -m tests.test_dry_run
```
All tests should pass.
  </verify>
  <done>
tests/test_dry_run.py has comprehensive test coverage for statistics, action labels, and cross-filesystem warnings. All tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 3: Verify full test suite passes</name>
  <files>tests/test_dry_run.py</files>
  <action>
1. Run the complete test suite to ensure no regressions:
   ```bash
   python3 run_tests.py
   ```

2. If any tests fail, analyze and fix the issues:
   - Check if test expectations match actual output format
   - Ensure mocking is done correctly
   - Verify test setup creates proper duplicate scenarios

3. Count total tests and verify test_dry_run.py has at least 12 tests covering:
   - Validation (2+ tests)
   - Banner (3+ tests)
   - Statistics (4+ tests)
   - Action labels (4+ tests)
   - Cross-filesystem (2+ tests)

4. Add any missing edge case tests:
   - Empty directories (no duplicates)
   - Single duplicate group
   - Multiple duplicate groups with varying sizes
  </action>
  <verify>
```bash
python3 run_tests.py 2>&1 | tail -10
# Should show all tests passing

python3 -m tests.test_dry_run -v 2>&1 | grep "^test_" | wc -l
# Should show 12+ tests
```
  </verify>
  <done>
Full test suite passes. test_dry_run.py has at least 12 tests covering all dry-run functionality. TEST-02 requirement complete.
  </done>
</task>

</tasks>

<verification>
1. `python3 run_tests.py` - all tests pass (including new dry-run tests)
2. `python3 -m tests.test_dry_run` - all dry-run tests pass
3. Test count: at least 12 new tests in test_dry_run.py
4. Coverage: validation, banner, statistics, action labels, cross-filesystem warnings
</verification>

<success_criteria>
- TEST-02: Unit tests for dry-run output formatting complete
- At least 12 tests covering all dry-run aspects
- All tests pass including existing test suite
- Tests follow established patterns from test_cli.py and test_master_directory.py
</success_criteria>

<output>
After completion, create `.planning/phases/02-dry-run-preview/02-04-SUMMARY.md`
</output>
