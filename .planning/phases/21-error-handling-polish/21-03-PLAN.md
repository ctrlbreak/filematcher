---
phase: 21-error-handling-polish
plan: 03
type: execute
wave: 2
depends_on: ["21-01"]
files_modified:
  - tests/test_error_handling.py
autonomous: true

must_haves:
  truths:
    - "Integration tests verify permission error flow end-to-end"
    - "Integration tests verify quit response returns correct exit code"
    - "Integration tests verify audit logger fail-fast behavior"
  artifacts:
    - path: "tests/test_error_handling.py"
      provides: "Integration tests for interactive_execute and audit logger"
      contains: "TestInteractiveExecuteErrorHandling"
  key_links:
    - from: "tests/test_error_handling.py"
      to: "filematcher/cli.py"
      via: "Tests call interactive_execute with mocked inputs"
      pattern: "interactive_execute"
    - from: "tests/test_error_handling.py"
      to: "filematcher/actions.py"
      via: "Tests verify create_audit_logger fail-fast"
      pattern: "create_audit_logger"
---

<objective>
Add integration tests for interactive error handling and audit logger fail-fast behavior.

Purpose: Test coverage for ERR-01 (permission errors continue) and audit logger fail-fast from v1.5 requirements.
Output: Integration tests appended to test_error_handling.py for interactive_execute and audit logger.
</objective>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/21-error-handling-polish/21-CONTEXT.md
@.planning/phases/21-error-handling-polish/21-01-SUMMARY.md
@filematcher/cli.py
@filematcher/actions.py
@tests/test_interactive.py
@tests/test_actions.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add interactive_execute integration tests</name>
  <files>tests/test_error_handling.py</files>
  <action>
Append integration test classes to tests/test_error_handling.py (created by Plan 21-02):

NOTE: Plan 21-02 must be executed first to create the base test file. This plan appends to it.

1. **TestInteractiveExecuteErrorHandling** (4 tests):
   - test_permission_error_displays_and_continues:
     * Create DuplicateGroup with 2 duplicates
     * Mock execute_action to return (False, "Permission denied") for first file, (True, None) for second
     * Call interactive_execute with 'a' (all) response
     * Assert failure_count=1, success_count=1, user_quit=False

   - test_oserror_on_file_size_displays_error:
     * Mock os.path.getsize to raise PermissionError("Permission denied")
     * Call interactive_execute
     * Assert formatter.format_file_error was called with the path and error

   - test_quit_response_returns_remaining_count:
     * Create 3 DuplicateGroups
     * Mock input to return 'y', 'q' (quit after first)
     * Assert remaining_count=1 (one group not processed)
     * Assert user_quit=True

   - test_keyboard_interrupt_sets_user_quit:
     * Mock input to raise KeyboardInterrupt
     * Call interactive_execute
     * Assert user_quit=True in return tuple

2. **TestExitCodes** (4 tests):
   - test_exit_success_when_no_failures: All succeed -> exit 0
   - test_exit_partial_when_some_failures: Some fail -> exit 2
   - test_exit_user_quit_on_q_response: User quits -> exit 130
   - test_exit_success_when_user_skips_all: User says 'n' to everything -> exit 0 (not an error)

Use test_base.BaseFileMatcherTest for temp directory management.
Mock input() for user responses.
Mock execute_action for controlled failure simulation.
  </action>
  <verify>
Run: `python3 -m unittest tests.test_error_handling.TestInteractiveExecuteErrorHandling tests.test_error_handling.TestExitCodes -v`
All 8 tests should pass.
  </verify>
  <done>
Integration tests verify permission error handling continues execution, quit returns remaining count, keyboard interrupt sets user_quit, and exit codes are correct for all scenarios.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add audit logger fail-fast tests</name>
  <files>tests/test_error_handling.py</files>
  <action>
Append audit logger test class to tests/test_error_handling.py:

**TestAuditLoggerFailFast** (2 tests):

1. test_audit_logger_exits_on_write_error:
   - Mock logging.FileHandler.__init__ to raise OSError("Permission denied")
   - Call create_audit_logger with any path
   - Assert SystemExit raised with code 2
   - Assert stderr contains "Cannot create audit log"

2. test_audit_logger_prints_helpful_message:
   - Same mock setup as above
   - Capture stderr
   - Assert message includes "Audit trail is required"

Use unittest.mock.patch for FileHandler mocking.
Use contextlib.redirect_stderr for stderr capture.
  </action>
  <verify>
Run: `python3 -m unittest tests.test_error_handling.TestAuditLoggerFailFast -v`
Both tests should pass.
  </verify>
  <done>
Audit logger fail-fast tests verify sys.exit(2) on file creation failure and helpful error message printed to stderr.
  </done>
</task>

</tasks>

<verification>
1. Run all error handling tests: `python3 -m unittest tests.test_error_handling -v`
2. Run full test suite: `python3 run_tests.py` - all tests pass
3. Verify test count: `python3 -m unittest tests.test_error_handling --collect-only 2>&1 | grep -c "test_"` should be ~19 total
</verification>

<success_criteria>
- [ ] TestInteractiveExecuteErrorHandling class exists with 4 tests
- [ ] TestExitCodes class exists with 4 tests
- [ ] TestAuditLoggerFailFast class exists with 2 tests
- [ ] Permission error simulation works (mock execute_action)
- [ ] Quit response correctly calculates remaining_count
- [ ] Audit logger fail-fast exits with code 2
- [ ] All tests pass including new integration tests
</success_criteria>

<output>
After completion, create `.planning/phases/21-error-handling-polish/21-03-SUMMARY.md`
</output>
