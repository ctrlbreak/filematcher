---
phase: 21-error-handling-polish
plan: 02
type: execute
wave: 2
depends_on: ["21-01"]
files_modified:
  - filematcher/formatters.py
  - filematcher/cli.py
  - tests/test_error_handling.py
autonomous: true

must_haves:
  truths:
    - "Final summary shows user decision counts (confirmed vs user-skipped)"
    - "Final summary shows execution results (succeeded vs failed)"
    - "Space saved displayed in both human-readable and bytes formats"
    - "Audit log path always shown in final summary"
    - "Exit code 2 returned on any execution failures in interactive mode"
    - "Tests cover permission errors, quit behavior, and comprehensive summary"
  artifacts:
    - path: "filematcher/formatters.py"
      provides: "Enhanced format_execution_summary with user decision counts"
      contains: "confirmed_count"
    - path: "filematcher/cli.py"
      provides: "Comprehensive summary display and correct exit codes"
      contains: "EXIT_PARTIAL"
    - path: "tests/test_error_handling.py"
      provides: "Unit tests for error handling and summary"
      min_lines: 150
  key_links:
    - from: "filematcher/cli.py"
      to: "filematcher/formatters.py"
      via: "format_execution_summary call with user decision counts"
      pattern: "confirmed_count.*user_skipped_count"
---

<objective>
Enhance execution summary with comprehensive counts and add tests for error handling.

Purpose: ERR-03 (comprehensive execution summary) from v1.5 requirements, plus test coverage for ERR-01/02.
Output: Updated summary format with user decisions and dual-format space savings, new test file for error handling.
</objective>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/21-error-handling-polish/21-CONTEXT.md
@.planning/phases/21-error-handling-polish/21-RESEARCH.md
@.planning/phases/21-error-handling-polish/21-01-SUMMARY.md
@filematcher/formatters.py
@filematcher/cli.py
@tests/test_interactive.py
@tests/test_actions.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Enhance format_execution_summary with comprehensive counts</name>
  <files>filematcher/formatters.py</files>
  <action>
Update format_execution_summary() signature in ActionFormatter ABC and both implementations to accept additional parameters:

```python
def format_execution_summary(
    self,
    success_count: int,
    failure_count: int,
    skipped_count: int,
    space_saved: int,
    log_path: str,
    failed_list: list[FailedOperation],
    confirmed_count: int = 0,      # NEW: groups user confirmed (y/a)
    user_skipped_count: int = 0    # NEW: groups user skipped (n)
) -> None:
```

Update TextActionFormatter.format_execution_summary():

Per CONTEXT.md - single compact block, three-way distinction:

```
Execution complete:
  User confirmed: {confirmed_count}
  User skipped: {user_skipped_count}
  Succeeded: {success_count}
  Failed: {failure_count}
  Already linked: {skipped_count}  # Only if > 0
  Space freed: {format_file_size(space_saved)} ({space_saved:,} bytes)
  Audit log: {log_path}

{If failed_list not empty:}
Failed files:
  X {path}: {error}
```

Note: Use red X marker (U+2717) for failed files list, matching format_file_error style.

Update JsonActionFormatter.format_execution_summary():

Add to execution object:
```python
self._data["execution"] = {
    "successCount": success_count,
    "failureCount": failure_count,
    "skippedCount": skipped_count,
    "spaceSavedBytes": space_saved,
    "logPath": log_path,
    "failures": failures,
    "userConfirmedCount": confirmed_count,      # NEW
    "userSkippedCount": user_skipped_count      # NEW
}
```
  </action>
  <verify>
Run: `python3 -c "from filematcher.formatters import TextActionFormatter; import inspect; sig = inspect.signature(TextActionFormatter.format_execution_summary); assert 'confirmed_count' in sig.parameters"`
Should pass (new parameter exists).
  </verify>
  <done>
format_execution_summary accepts and displays user decision counts. Text output shows three-way distinction (confirmed/skipped/failed). Space saved shows both human-readable and bytes.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update CLI to pass user decision counts and use EXIT_PARTIAL</name>
  <files>filematcher/cli.py</files>
  <action>
NOTE: This task depends on Plan 21-01 which defines EXIT_SUCCESS, EXIT_PARTIAL, EXIT_USER_QUIT constants and updates interactive_execute() to return 9-element tuple with remaining_count and user_quit.

Update ALL THREE call sites to format_execution_summary() to pass confirmed_count and user_skipped_count:

1. **Line ~648 (JSON batch mode):**
   After batch execution completes, pass confirmed_count=len(master_results), user_skipped_count=0 since all groups are auto-confirmed.

2. **Line ~694 (Text batch mode with --yes flag):**
   After batch_execute completes, pass confirmed_count=len(master_results), user_skipped_count=0.

3. **Line ~742 (Interactive mode after interactive_execute returns):**
   Pass confirmed_count and user_skipped_count from interactive_execute return tuple.

For all call sites:
```python
action_formatter.format_execution_summary(
    success_count=success_count,
    failure_count=failure_count,
    skipped_count=skipped_count,
    space_saved=space_saved,
    log_path=str(actual_log_path),
    failed_list=failed_list,
    confirmed_count=confirmed_count,
    user_skipped_count=user_skipped_count
)
```

Update exit code logic in main() after interactive_execute (non-quit case):

Per CONTEXT.md: "Exit code 2 on any errors in batch mode (partial success = exit 2)"

Change from using determine_exit_code() to:
```python
if failure_count > 0:
    return EXIT_PARTIAL  # 2 - partial success with errors
return EXIT_SUCCESS  # 0 - full success
```

Note: User skipping via 'n' is NOT an error - exit 0 if user skipped but no failures.
Exit 2 only when actual execution failures occurred.
  </action>
  <verify>
Run: `python3 -c "from filematcher.cli import main; import sys; sys.argv = ['filematcher', '--help']; main()"` - should show help without error, confirming module loads correctly.
  </verify>
  <done>
All three format_execution_summary calls (JSON batch ~648, text batch ~694, interactive ~742) include user decision counts. Exit code 2 returned when any execution failures occur. User skipping via 'n' does not cause error exit.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create formatter and unit tests for error handling</name>
  <files>tests/test_error_handling.py</files>
  <action>
Create new test file with unit tests for Phase 21 formatter methods and basic error handling:

```python
"""Tests for error handling and execution summaries (Phase 21)."""

import unittest
import tempfile
import os
from pathlib import Path
from unittest.mock import patch, MagicMock
from io import StringIO

from filematcher.formatters import TextActionFormatter, JsonActionFormatter
from filematcher.colors import ColorConfig, ColorMode
from filematcher.types import Action, DuplicateGroup, FailedOperation
from filematcher.cli import EXIT_SUCCESS, EXIT_PARTIAL, EXIT_USER_QUIT

from tests.test_base import BaseFileMatcherTest
```

Test classes to create:

1. **TestFormatFileError** (2 tests):
   - test_text_format_file_error_output: Verify text output includes path and error
   - test_json_format_file_error_accumulates: Verify JSON accumulates in errors array

2. **TestFormatQuitSummary** (3 tests):
   - test_text_format_quit_summary_all_fields: Verify all fields present
   - test_text_format_quit_summary_zero_space: Verify no "Freed" line when space_saved=0
   - test_json_format_quit_summary_structure: Verify JSON quit status structure

3. **TestExecutionSummaryEnhanced** (3 tests):
   - test_text_summary_shows_user_decisions: Verify confirmed/skipped counts in output
   - test_text_summary_shows_dual_space_format: Verify both "1.2 MB" and bytes format
   - test_json_summary_includes_user_counts: Verify JSON has userConfirmedCount, userSkippedCount

4. **TestExitCodeConstants** (1 test):
   - test_exit_code_values: Verify EXIT_SUCCESS=0, EXIT_PARTIAL=2, EXIT_USER_QUIT=130

Use ColorMode.NEVER for predictable text output.
Use StringIO and patch for stdout capture.

Total: ~80-100 lines, 9 tests focused on formatter output.
  </action>
  <verify>
Run: `python3 -m unittest tests.test_error_handling -v`
All 9 tests should pass.
  </verify>
  <done>
Formatter tests cover format_file_error, format_quit_summary, and enhanced format_execution_summary. Exit code constants verified. All tests pass.
  </done>
</task>

</tasks>

<verification>
1. Run full test suite: `python3 run_tests.py` - all tests pass (including new test_error_handling.py)
2. Run new tests specifically: `python3 -m unittest tests.test_error_handling -v`
3. Manual verification: Run interactive mode, cause a permission error, verify inline display
4. Manual verification: Run interactive mode, press 'q', verify summary shows remaining count
</verification>

<success_criteria>
- [ ] format_execution_summary() accepts confirmed_count and user_skipped_count parameters
- [ ] TextActionFormatter shows user decisions in summary (confirmed/skipped)
- [ ] TextActionFormatter shows space in dual format (human-readable and bytes)
- [ ] JsonActionFormatter includes userConfirmedCount and userSkippedCount
- [ ] CLI passes user decision counts to format_execution_summary() at all 3 call sites
- [ ] Exit code 2 returned when any failures occur
- [ ] Exit code 0 returned when user skips all via 'n' (not an error)
- [ ] tests/test_error_handling.py exists with 9 formatter/unit tests
- [ ] All tests pass including new error handling tests
</success_criteria>

<output>
After completion, create `.planning/phases/21-error-handling-polish/21-02-SUMMARY.md`
</output>
