---
phase: 21-error-handling-polish
plan: 02
type: execute
wave: 2
depends_on: ["21-01"]
files_modified:
  - filematcher/formatters.py
  - filematcher/cli.py
  - tests/test_error_handling.py
autonomous: true

must_haves:
  truths:
    - "Final summary shows user decision counts (confirmed vs user-skipped)"
    - "Final summary shows execution results (succeeded vs failed)"
    - "Space saved displayed in both human-readable and bytes formats"
    - "Audit log path always shown in final summary"
    - "Exit code 2 returned on any execution failures in interactive mode"
    - "Tests cover permission errors, quit behavior, and comprehensive summary"
  artifacts:
    - path: "filematcher/formatters.py"
      provides: "Enhanced format_execution_summary with user decision counts"
      contains: "confirmed_count"
    - path: "filematcher/cli.py"
      provides: "Comprehensive summary display and correct exit codes"
      contains: "EXIT_PARTIAL"
    - path: "tests/test_error_handling.py"
      provides: "Unit tests for error handling and summary"
      min_lines: 150
  key_links:
    - from: "filematcher/cli.py"
      to: "filematcher/formatters.py"
      via: "format_execution_summary call with user decision counts"
      pattern: "confirmed_count.*user_skipped_count"
---

<objective>
Enhance execution summary with comprehensive counts and add tests for error handling.

Purpose: ERR-03 (comprehensive execution summary) from v1.5 requirements, plus test coverage for ERR-01/02.
Output: Updated summary format with user decisions and dual-format space savings, new test file for error handling.
</objective>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/21-error-handling-polish/21-CONTEXT.md
@.planning/phases/21-error-handling-polish/21-RESEARCH.md
@.planning/phases/21-error-handling-polish/21-01-SUMMARY.md
@filematcher/formatters.py
@filematcher/cli.py
@tests/test_interactive.py
@tests/test_actions.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Enhance format_execution_summary with comprehensive counts</name>
  <files>filematcher/formatters.py</files>
  <action>
Update format_execution_summary() signature in ActionFormatter ABC and both implementations to accept additional parameters:

```python
def format_execution_summary(
    self,
    success_count: int,
    failure_count: int,
    skipped_count: int,
    space_saved: int,
    log_path: str,
    failed_list: list[FailedOperation],
    confirmed_count: int = 0,      # NEW: groups user confirmed (y/a)
    user_skipped_count: int = 0    # NEW: groups user skipped (n)
) -> None:
```

Update TextActionFormatter.format_execution_summary():

Per CONTEXT.md - single compact block, three-way distinction:

```
Execution complete:
  User confirmed: {confirmed_count}
  User skipped: {user_skipped_count}
  Succeeded: {success_count}
  Failed: {failure_count}
  Already linked: {skipped_count}  # Only if > 0
  Space freed: {format_file_size(space_saved)} ({space_saved:,} bytes)
  Audit log: {log_path}

{If failed_list not empty:}
Failed files:
  X {path}: {error}
```

Note: Use red X marker (U+2717) for failed files list, matching format_file_error style.

Update JsonActionFormatter.format_execution_summary():

Add to execution object:
```python
self._data["execution"] = {
    "successCount": success_count,
    "failureCount": failure_count,
    "skippedCount": skipped_count,
    "spaceSavedBytes": space_saved,
    "logPath": log_path,
    "failures": failures,
    "userConfirmedCount": confirmed_count,      # NEW
    "userSkippedCount": user_skipped_count      # NEW
}
```
  </action>
  <verify>
Run: `python3 -c "from filematcher.formatters import TextActionFormatter; import inspect; sig = inspect.signature(TextActionFormatter.format_execution_summary); assert 'confirmed_count' in sig.parameters"`
Should pass (new parameter exists).
  </verify>
  <done>
format_execution_summary accepts and displays user decision counts. Text output shows three-way distinction (confirmed/skipped/failed). Space saved shows both human-readable and bytes.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update CLI to pass user decision counts and use EXIT_PARTIAL</name>
  <files>filematcher/cli.py</files>
  <action>
1. Update all calls to format_execution_summary() to pass confirmed_count and user_skipped_count:

For interactive mode (after interactive_execute returns):
```python
action_formatter.format_execution_summary(
    success_count=success_count,
    failure_count=failure_count,
    skipped_count=skipped_count,
    space_saved=space_saved,
    log_path=str(actual_log_path),
    failed_list=failed_list,
    confirmed_count=confirmed_count,
    user_skipped_count=user_skipped_count
)
```

For batch mode (--yes flag): Pass confirmed_count=len(master_results), user_skipped_count=0 since all groups are auto-confirmed.

2. Update exit code logic in main() after interactive_execute:

Per CONTEXT.md: "Exit code 2 on any errors in batch mode (partial success = exit 2)"

Change from using determine_exit_code() to:
```python
if failure_count > 0:
    return EXIT_PARTIAL  # 2 - partial success with errors
return EXIT_SUCCESS  # 0 - full success
```

Note: User skipping via 'n' is NOT an error - exit 0 if user skipped but no failures.
Exit 2 only when actual execution failures occurred.

3. For JSON batch mode, also update to pass confirmed_count and user_skipped_count.
  </action>
  <verify>
Run: `python3 -c "from filematcher.cli import main; import sys; sys.argv = ['filematcher', '--help']; main()"` - should show help without error, confirming module loads correctly.
  </verify>
  <done>
All format_execution_summary calls include user decision counts. Exit code 2 returned when any execution failures occur. User skipping via 'n' does not cause error exit.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create comprehensive error handling tests</name>
  <files>tests/test_error_handling.py</files>
  <action>
Create new test file with tests for Phase 21 error handling:

```python
"""Tests for error handling and execution summaries (Phase 21)."""

import unittest
import tempfile
import os
from pathlib import Path
from unittest.mock import patch, MagicMock
from io import StringIO

from filematcher.formatters import TextActionFormatter, JsonActionFormatter
from filematcher.colors import ColorConfig, ColorMode
from filematcher.types import Action, DuplicateGroup, FailedOperation
from filematcher.cli import (
    interactive_execute, EXIT_SUCCESS, EXIT_PARTIAL, EXIT_USER_QUIT
)
from filematcher.actions import create_audit_logger

from tests.test_base import BaseFileMatcherTest
```

Test classes to create:

1. TestFormatFileError:
   - test_text_format_file_error_output: Verify text output includes path and error
   - test_json_format_file_error_accumulates: Verify JSON accumulates in errors array

2. TestFormatQuitSummary:
   - test_text_format_quit_summary_all_fields: Verify all fields present
   - test_text_format_quit_summary_zero_space: Verify no "Freed" line when space_saved=0
   - test_json_format_quit_summary_structure: Verify JSON quit status structure

3. TestInteractiveExecuteErrorHandling:
   - test_permission_error_displays_and_continues: Mock execute_action to return permission error for first file, success for second. Verify failure_count=1, success_count=1.
   - test_oserror_on_file_size_displays_error: Mock os.path.getsize to raise PermissionError. Verify format_file_error called.
   - test_quit_response_returns_remaining_count: Simulate 'q' response, verify remaining_count correct.
   - test_keyboard_interrupt_sets_user_quit: Raise KeyboardInterrupt in prompt, verify user_quit=True.

4. TestExecutionSummaryEnhanced:
   - test_text_summary_shows_user_decisions: Verify confirmed/skipped counts in output
   - test_text_summary_shows_dual_space_format: Verify both "1.2 MB" and bytes format
   - test_json_summary_includes_user_counts: Verify JSON has userConfirmedCount, userSkippedCount

5. TestExitCodes:
   - test_exit_success_when_no_failures: All succeed -> exit 0
   - test_exit_partial_when_some_failures: Some fail -> exit 2
   - test_exit_user_quit_on_q_response: User quits -> exit 130
   - test_exit_success_when_user_skips_all: User says 'n' to everything -> exit 0 (not an error)

6. TestAuditLoggerFailFast:
   - test_audit_logger_exits_on_write_error: Mock FileHandler to raise OSError, verify sys.exit(2) called

Use test_base.BaseFileMatcherTest for tests needing temp directories.
Use StringIO and patch for stdout capture.
Use ColorMode.NEVER for predictable text output.
  </action>
  <verify>
Run: `python3 -m pytest tests/test_error_handling.py -v` or `python3 -m unittest tests.test_error_handling -v`
All tests should pass.
  </verify>
  <done>
Comprehensive test coverage for error handling: inline error display, quit summary, user decision counts, exit codes, and audit logger fail-fast. All tests pass.
  </done>
</task>

</tasks>

<verification>
1. Run full test suite: `python3 run_tests.py` - all tests pass (including new test_error_handling.py)
2. Run new tests specifically: `python3 -m unittest tests.test_error_handling -v`
3. Manual verification: Run interactive mode, cause a permission error, verify inline display
4. Manual verification: Run interactive mode, press 'q', verify summary shows remaining count
</verification>

<success_criteria>
- [ ] format_execution_summary() accepts confirmed_count and user_skipped_count parameters
- [ ] TextActionFormatter shows user decisions in summary (confirmed/skipped)
- [ ] TextActionFormatter shows space in dual format (human-readable and bytes)
- [ ] JsonActionFormatter includes userConfirmedCount and userSkippedCount
- [ ] CLI passes user decision counts to format_execution_summary()
- [ ] Exit code 2 returned when any failures occur
- [ ] Exit code 0 returned when user skips all via 'n' (not an error)
- [ ] tests/test_error_handling.py exists with 15+ tests
- [ ] All tests pass including new error handling tests
</success_criteria>

<output>
After completion, create `.planning/phases/21-error-handling-polish/21-02-SUMMARY.md`
</output>
